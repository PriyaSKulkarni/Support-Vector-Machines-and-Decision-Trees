{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "3b)"
      ],
      "metadata": {
        "id": "lxwOlgS-KWky"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vkEw_3GHf4V",
        "outputId": "ebb503e6-95a2-4aaf-e700-a6311b182c9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "         height     diameter       weight          hue      class\n",
            "0   0.117820005  0.138365287  0.376838312  3.255943211   Plastic \n",
            "1   0.149398845  0.137884636  0.458635784  4.887697997   Plastic \n",
            "2   0.092846989   0.08946483  0.398115904  3.314872309     Metal \n",
            "3   0.063493171         0.03  0.110605349  4.290583456     Metal \n",
            "4   0.132147759         0.15  0.366192424  2.642717324   Plastic \n",
            "5   0.134386556         0.15         0.75  2.549392832   Ceramic \n",
            "6   0.077117679  0.030795497  0.132188862  1.061354957     Metal \n",
            "7   0.088096721  0.093578829  0.514391815  2.807311094   Ceramic \n",
            "8   0.088439453   0.13763057  0.709234615  2.234895655   Ceramic \n",
            "9   0.146148564  0.135840972  0.451317027  4.177574005   Plastic \n",
            "10   0.09265452  0.080861592  0.567104482  1.641808644   Ceramic \n",
            "11  0.080698615  0.034717811          0.1   1.79095088   Plastic \n",
            "              height            diameter             weight               hue\n",
            "0   0.10085325871588    0.10347665370087   0.66000055127054   3.1061777201591\n",
            "1  0.097520805629366     0.1201052695695               0.75   1.4520706957674\n",
            "2  0.070973086761957   0.088622489628388   0.10604947426549   3.5044594187921\n",
            "3   0.11843514045485                0.15   0.33839714871863   5.6111022342157\n"
          ]
        }
      ],
      "source": [
        "# importing the required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# I have used my google drive to load the training and testing data\n",
        "# Upload the dataset file in your google drive and change the path to run the below line\n",
        "# Using the dataset of 2a of howework 1 only in 3b problem\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "train_data= pd.read_csv('/content/drive/MyDrive/ML/Priyadataset.csv',dtype='object')\n",
        "print(train_data)\n",
        "test_data=pd.read_csv('/content/drive/MyDrive/ML/PriyaTestdata.csv',dtype='object')\n",
        "print(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = train_data.iloc[:,:-1]\n",
        "Y = train_data.iloc[:,-1]\n",
        "x_train = X.values\n",
        "y_train = Y.values.reshape(-1,1)\n",
        "x_train=x_train.astype('float64')\n",
        "X_t= test_data.iloc[:,:-1]\n",
        "x_test = X_t.values\n",
        "x_test= x_test.astype('float64')"
      ],
      "metadata": {
        "id": "oHQ-GMfqLC3U"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing the decision tree"
      ],
      "metadata": {
        "id": "bmiIHqNGX0rM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defining the node ( basically each tree node)\n",
        "# A = attribute  \n",
        "# T = threshold\n",
        "# l = left node\n",
        "# r = right node\n",
        "# Ig = information gain\n",
        "# V = value\n",
        "class Node():\n",
        "    def __init__(self, A=None, T=None, l=None, r=None, Ig=None, V=None):\n",
        "        self.A = A\n",
        "        self.T = T\n",
        "        self.l = l\n",
        "        self.r= r\n",
        "        self.Ig = Ig\n",
        "        self.V = V"
      ],
      "metadata": {
        "id": "FalQ8PHXKrll"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# decision tree implementation\n",
        "class DecisionTree():\n",
        "  # initializing the minimum sample default value as 2 and maximum depth value as 8\n",
        "    def __init__(self, minimum_sample=2, maximum_depth=8):\n",
        "        self.root = None\n",
        "        self.minimum_sample = minimum_sample\n",
        "        self.maximum_depth = maximum_depth\n",
        "   \n",
        "    # caluculating the entropy for the given value with log base of 10\n",
        "    def Calculate_entropy(self, y_value):\n",
        "        y_label = np.unique(y_value)\n",
        "        #print(y_label)\n",
        "        entropy = 0\n",
        "        # for each attribute in the y_label\n",
        "        for att in y_label:\n",
        "            att_p = len(y_value[y_value == att]) / len(y_value)\n",
        "            entropy += -att_p * np.log10(att_p)\n",
        "            #print(entropy)\n",
        "        # return the entropy value\n",
        "        return entropy   \n",
        "\n",
        "    # calculating the information gain in this function\n",
        "    def calulate_info_gain(self, data, l_, r_):\n",
        "       #caluclate the total datapoints in dataset \n",
        "        tot = len(l_) + len(r_)\n",
        "        # how many datapoints in left tree / total length\n",
        "        prob_left= len(l_) / tot\n",
        "         # how many datapoints in right tree / total length\n",
        "        prob_right = len(r_) / tot\n",
        "        # calculate the left and right entropy by calling the function\n",
        "        entropy_left = self.Calculate_entropy(l_)\n",
        "        entropy_right= self.Calculate_entropy(r_)\n",
        "        # value to split the data\n",
        "        #print(entropy_left)\n",
        "        #print(entropy_right)\n",
        "        split_data = (prob_left * entropy_left) + (prob_right * entropy_right)\n",
        "        #print(self.Calculate_entropy(data))\n",
        "        # calculating the info gain\n",
        "        infogain = self.Calculate_entropy(data) - split_data\n",
        "        #print(infogain)\n",
        "        return infogain\n",
        "\n",
        "    # best split is calculated by calculating the thershold value by taking the best average of 2 data points in whole dataset\n",
        "    #  returns the best split thershold value node\n",
        "    def best_split(self, data, rows, cols):\n",
        "        best_s = {}\n",
        "        #initialzing the information gain to infinity \n",
        "        info_gain = -float('inf')\n",
        "        for col in range(cols):\n",
        "            value = data[:, col]\n",
        "            _t = []\n",
        "            #print(_t)\n",
        "            # calulating the thershold by taking 2 datapoints and there average aand storing in _t variable\n",
        "            for i in range(len(value)):\n",
        "                if i != 0:\n",
        "                    current = value[i]\n",
        "                    next = value[i - 1]\n",
        "                    split = (current + next) / 2\n",
        "                    _t.append(split)\n",
        "            #print(_t)\n",
        "            # spliting the data based on each thershold value and calculating which split gives the maximum information gain\n",
        "            for t in _t:\n",
        "              # divide the dataset based on the bestsplit  e \n",
        "                left = np.array([i for i in data if i[col]<=t])\n",
        "                #print(left)\n",
        "                right = np.array([i for i in data if i[col]>t])\n",
        "                #print(right)\n",
        "                if len(left)>0 and len(right)>0:\n",
        "                    y, left_y, right_y = data[:, 0:4], left[:, 0:4], right[:, 0:4]\n",
        "                    # calling information gain method to check the value\n",
        "                    IG = self.calulate_info_gain(y, left_y, right_y)\n",
        "                    #print(IG)\n",
        "                    if IG>info_gain:\n",
        "                        best_s[\"A\"] = col\n",
        "                        best_s[\"T\"] = t\n",
        "                        best_s[\"l\"] = left\n",
        "                        best_s[\"r\"] = right\n",
        "                        best_s[\"Ig\"] = IG\n",
        "                        info_gain = IG \n",
        "            _t = []\n",
        "            #print(_t)\n",
        "            #print(best_s)\n",
        "        return best_s\n",
        "\n",
        "    # building the tree \n",
        "    # dividing the into left and right tree  by criteria of maximum information gain and returing the node\n",
        "    def tree_build(self, data, depth=0):        \n",
        "        x_data, y_data = data[:,0:4], data[:,4]\n",
        "        rows, cols = np.shape(x_data)\n",
        "        # checking whether the minimum sample and maximum depth number is satisfied\n",
        "        if rows>=self.minimum_sample and depth<=self.maximum_depth:\n",
        "          # calling the best split function to divide it into left and right tree\n",
        "            best_s = self.best_split(data, rows, cols)\n",
        "            if best_s[\"Ig\"]>0:\n",
        "               # using the bestsplit data left and right tree is build by calling the function recursively\n",
        "                left_tree = self.tree_build(best_s[\"l\"], depth+1)\n",
        "                right_tree = self.tree_build(best_s[\"r\"], depth+1)\n",
        "                return Node(best_s[\"A\"], best_s[\"T\"], left_tree, right_tree, best_s[\"Ig\"])\n",
        "        # after continous manipulation of left and right tree --> we come with the left node\n",
        "        leaf_n = max(list(y_data), key=list(y_data).count)\n",
        "        #print(left_n)\n",
        "        return Node(V=leaf_n)\n",
        "\n",
        "    # training the dataset for decision tree\n",
        "    def fit_funct(self, X_value, Y_value):\n",
        "        whole_dataset = np.concatenate((X_value, Y_value), axis=1)\n",
        "        # calling the function tree_build to build the decision tree\n",
        "        self.root = self.tree_build(whole_dataset)\n",
        "    \n",
        "    # prediction function calls the classification to traverse through the built tree\n",
        "    def class_predict(self, x_value):\n",
        "      # initially it is started with root node hence self.root is passed\n",
        "        pred = [self.classification(x, self.root) for x in x_value]\n",
        "        return pred\n",
        "\n",
        "    # classification function helps to traverse through the tree to match the attribute  and predict the required output\n",
        "    def classification(self, x_v, d_tree):\n",
        "      # for 1 level tree\n",
        "        if d_tree.V!=None: return d_tree.V\n",
        "        #checking for the attribute values\n",
        "        value = x_v[d_tree.A]\n",
        "        #print(value)\n",
        "        # using <= value to divide the threshold value to left or right tree\n",
        "        if value <= d_tree.T:\n",
        "            return self.classification(x_v, d_tree.l)\n",
        "        else:\n",
        "            return self.classification(x_v, d_tree.r)\n",
        "    \n",
        "    # Calculating the accuracy value using the predicted value and actual value\n",
        "    def accuracy(self, y_test, y_pred):\n",
        "      correct = 0\n",
        "      for i in range(len(y_test)):\n",
        "        if y_test[i] == y_pred[i]:\n",
        "            correct += 1\n",
        "      #print(correct)\n",
        "      return correct/float(len(y_test)) * 100.0 "
      ],
      "metadata": {
        "id": "w1cBqF0RIWx3"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building the decision tree based on the predetermined depth of 3\n",
        "\n"
      ],
      "metadata": {
        "id": "WgIuoumPX6Iu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"the pre-determined depth considered is 3\")\n",
        "dec_tree = DecisionTree(minimum_sample=2, maximum_depth=3)\n",
        "dec_tree.fit_funct(x_train,y_train)\n",
        "y_pred = dec_tree.class_predict(x_train)\n",
        "print('Accuracy for the training data: ',dec_tree.accuracy(y_train, y_pred))\n",
        "y_pred_test = dec_tree.class_predict(x_test)\n",
        "print('Values predicted for the test data:', y_pred_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTsC2fycN3xZ",
        "outputId": "895b3e22-8003-425a-8445-b94fd375a277"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the pre-determined depth considered is 3\n",
            "Accuracy for the training data:  100.0\n",
            "Values predicted for the test data: [' Metal ', ' Ceramic ', ' Metal ', ' Plastic ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3c)"
      ],
      "metadata": {
        "id": "wfOVLGpHYPM3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "deriving trees of depths 1 until 8 and evaluating the accuracy of\n",
        "each of the 8 resulting trees for the training samples and for the test set"
      ],
      "metadata": {
        "id": "c_4mFLmHYQjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dividing the large training data set into a test set\n",
        "# containing the first 6 data items of each material type and a training set containing the remaining data points\n",
        "# stored in the form of csv in the google drive\n",
        "# I have used my google drive to load the training and testing data\n",
        "# Upload the dataset file in your google drive and change the path to run the below line\n",
        "train_data= pd.read_csv('/content/drive/MyDrive/ML/Priyadataset3cTrain.csv',dtype='object')\n",
        "print(train_data)\n",
        "test_data=pd.read_csv('/content/drive/MyDrive/ML/Priyadataset3cTest.csv',dtype='object')\n",
        "print(test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXN2QY_3RFik",
        "outputId": "9f289b41-167f-4d18-8d0e-665220d17b9d"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          height     diameter       weight          hue      class\n",
            "0    0.081220302  0.030216702  0.143960586  3.269462663   Plastic \n",
            "1    0.123647141  0.116415097         0.75  1.972557371   Ceramic \n",
            "2    0.089885502  0.074007617  0.243078037  2.245829339   Plastic \n",
            "3    0.102275618  0.105571657  0.675379468  3.014154131   Ceramic \n",
            "4    0.130108528  0.126018429         0.75  2.568700685   Ceramic \n",
            "..           ...          ...          ...          ...        ...\n",
            "98   0.153248087  0.130163833  0.401184574  2.179239163   Plastic \n",
            "99   0.120215228  0.116073582  0.338827775  0.876866304   Plastic \n",
            "100  0.075569171  0.072067954    0.1616056   3.33827839   Plastic \n",
            "101  0.131371291  0.091788908  0.278141251  2.383392581   Plastic \n",
            "102   0.12825405  0.148201394  0.455335366  2.686607561   Plastic \n",
            "\n",
            "[103 rows x 5 columns]\n",
            "         height     diameter       weight          hue      class\n",
            "0   0.124937779  0.131903758  0.386168249  2.951766979   Plastic \n",
            "1   0.144921374  0.125007466         0.75  3.437875515   Ceramic \n",
            "2   0.071892498         0.03  0.163176187  4.052152861   Plastic \n",
            "3   0.084260612   0.03768188  0.231511658  6.283185307   Ceramic \n",
            "4          0.07   0.09034795  0.191776997  2.125503851   Plastic \n",
            "5   0.099799767  0.102694341  0.636165293  3.535851838   Ceramic \n",
            "6          0.05         0.03          0.1  4.301176935   Ceramic \n",
            "7    0.07237162  0.054525468  0.128053991  4.422755319     Metal \n",
            "8    0.07578485         0.03          0.1  1.401925743   Plastic \n",
            "9   0.059473196  0.066294735  0.221788216  2.327092563     Metal \n",
            "10  0.125864226  0.122610425         0.75  1.315462534   Ceramic \n",
            "11  0.161399649  0.126263296  0.431165899  2.490957279   Plastic \n",
            "12  0.074933452  0.030261702          0.1  3.732772495   Plastic \n",
            "13  0.113301912  0.101372791  0.489402406  2.873577049     Metal \n",
            "14  0.091344351  0.089602319  0.372979874  3.772084978     Metal \n",
            "15  0.073145642  0.051687026  0.219246045  5.007641933   Ceramic \n",
            "16  0.069363738  0.079143399  0.216742732  2.122002092     Metal \n",
            "17  0.102580501  0.105407344   0.47923086  2.910962306     Metal \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = train_data.iloc[:,:-1]\n",
        "Y = train_data.iloc[:,-1]\n",
        "x_train = X.values\n",
        "y_train = Y.values.reshape(-1,1)\n",
        "x_train=x_train.astype('float64')\n",
        "X_t= test_data.iloc[:,:-1]\n",
        "Y_t = test_data.iloc[:,-1]\n",
        "x_test = X_t.values\n",
        "y_test = Y_t.values.reshape(-1,1)\n",
        "x_test= x_test.astype('float64')"
      ],
      "metadata": {
        "id": "d-Qqy8fPRkI0"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1, 9):\n",
        "    print('For Depth value : ', i)\n",
        "    dt = DecisionTree(minimum_sample=2, maximum_depth=i)\n",
        "    dt.fit_funct(x_train,y_train)\n",
        "    y_pred = dt.class_predict(x_train)\n",
        "    print('Accuracy on training dataset: ', dt.accuracy(y_train, y_pred))   \n",
        "    y_pred_test = dt.class_predict(x_test)\n",
        "    print('Accuracy on test dataset: ', dt.accuracy(y_test, y_pred_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jTkyVhyK_CW",
        "outputId": "4a6991f9-003d-499f-f953-20944825b7de"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For Depth value :  1\n",
            "Accuracy on training dataset:  69.90291262135922\n",
            "Accuracy on test dataset:  50.0\n",
            "For Depth value :  2\n",
            "Accuracy on training dataset:  69.90291262135922\n",
            "Accuracy on test dataset:  50.0\n",
            "For Depth value :  3\n",
            "Accuracy on training dataset:  73.7864077669903\n",
            "Accuracy on test dataset:  61.111111111111114\n",
            "For Depth value :  4\n",
            "Accuracy on training dataset:  79.6116504854369\n",
            "Accuracy on test dataset:  55.55555555555556\n",
            "For Depth value :  5\n",
            "Accuracy on training dataset:  84.46601941747572\n",
            "Accuracy on test dataset:  55.55555555555556\n",
            "For Depth value :  6\n",
            "Accuracy on training dataset:  100.0\n",
            "Accuracy on test dataset:  55.55555555555556\n",
            "For Depth value :  7\n",
            "Accuracy on training dataset:  100.0\n",
            "Accuracy on test dataset:  55.55555555555556\n",
            "For Depth value :  8\n",
            "Accuracy on training dataset:  100.0\n",
            "Accuracy on test dataset:  55.55555555555556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By comparing the accuracy of training and test dataset we can say that the best decision tree is with the depth 3 because it has the maximum test data accuracy of 61.111% "
      ],
      "metadata": {
        "id": "yr9dWPC6axqq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see depth 3 accuracy of test data is 61.11%, increasing the depth value led to the decrease in the test data accuracy \n",
        "\n",
        "so from this we can conclude that from depth 4 onwards the decision tree tried to overfit "
      ],
      "metadata": {
        "id": "dndmRxawbUM7"
      }
    }
  ]
}